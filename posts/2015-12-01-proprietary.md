# Where is the Iron Mountain for open data?
by [Karissa McKelvey](http://karissa.github.io)

**Proprietary services such as Google, GitHub, Dropbox, PLOS, Figshare, and AWS provide data hosting and sharing at a low cost. They've become the most popular platforms for data sharing in science. However, using them can be dangerous -- when the money runs out, so does your data.**

Imagine a scenario thirty years in the future, when most of the world's scientific data has been hosted on DropBox, and the company closes its doors.
When this happens, companies will often give users a chance to download that data before it is deleted permanently. Although many people will download the data, scientists do not have the same incentive to download and store their data for the simple purpose of archiving and reproducibility. We could predict that most scientists will not keep their data around. Truthfully, this is already a serious problem without a DropBox closure, discovered when the question "Where are the original scripts and data you used?" is answered with a resounding "I don't know" or referral to another person. Much of the content produced on these closed, proprietary will be deleted, valuable scientific artifacts lost forever.

It is expected that some, or most, of the companies around today won't exist in fifty years. The average lifespan of a company has decreased from 67 years in the 1920s to just [15 years today](http://www.bbc.com/news/business-16611040). With this expectation, as a thought experiment, if Galileo stored photos from his telescope on DropBox in 1610, someone would have had to copy the data and host it again anywhere from 11 to 33 times in order for it to be available today. Sound familiar? For decades, we've been copying paper records and storing them in literal [mountains made of iron](http://www.ironmountain.com/) on microfilm, a hardy material that has a life expectancy of [a few hundred years](https://en.wikipedia.org/wiki/Microform). When data is copied, it has higher redundancy, increasing the chance that it will be around forever. In the digital context, it seems to follow that *copying data* from a digital hosting service should not be prevented. If we want data to be around forever, it should be free to be retrieved and copied at any point in time.

## So how did we get here in the first place?

Unfortunately, many proprietary hosting services do not like it when their users can copy and re-host data. Many hosting companies will offer private installations on an organization's own hardware and databases. Even so, these clients may not copy, distribute, study, change, or improve the software without permission from the company. This is how service hosting companies make money in the long-run -- once the users buy in to the software and increase adoption, it is difficult (or impossible) to move that data to another service. Proprietary services use code that is closed source for this purpose, so that users are incapable, legally, of running their own copy of the software and data.

To make matters worse, even if the hosting service prides itself on its ease of data transfer, it can still take tremendous effort to move data off of a proprietary service. Amazon encourages users by making it free to upload data to cloud storage, but it charges the organization every time the data needs to be downloaded. Bandwidth can become expensive, especially if we're talking about very large datasets (on the order of terrabytes). Who will pay for this data transfer cost? The individual researcher will unlikely have that money, and it is unsustainable to expect the funds to be around forever.

Although unfortunate, this market landscape is not necessarily the fault of companies, and a data service should not be considered harmful just because it is a business. I don't want to create a pitchfork movement against all business hosting services. I simply want to point out that we need to start supporting and using services that are open source (free as in freedom, not beer) and allow users to copy and re-host data. This should be considered in the design as not only a necessity, but an inevitability in the use of the software. Even though this copying and re-hosting data might not good for a centralized business model, it is a sure way to create redundancy and ensure scientific materials will be around forever.

## Start here: open data as common knowledge

As open data publication becomes normalized, we need to concern ourselves heavily with the method -- and that's where tools like Dat can come in. Dat is a distributed data sharing tool that is has been open source since day one. We have designed the tool explicitly for **editing, copying, re-hosting, and redistributing data**. The data hosting model will work like SETI@Home, where any user or organization can run a node that will offer bandwidth to the network to re-host and share data. This distribution model for data will enable libraries, non profits, scientific societies, universities, news organizations, and open data scientists to all participate in the open data network, drastically increasing redundancy and ensuring longevity compared to the current centralized model.

We're convinced that this distributed data model is the most durable for ensuring that public knowledge stays accessible. Brewster Khale of the Internet Archive has called for an [internet that 'locks' openness into how it works and operates](http://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/) through distributing data over a peer to peer network. We support this notion -- with a distributed, peer to peer data sharing tool, power over the data is distributed. This method of distribution is harder to monetize a single corporate entity compared to a centralized, corporate model. The market simply does not have the incentive structures to build this kind of distributed database for public knowledge.

Libraries, on the other hand, are in a great position to be shepherds of public knowledge. In practice, copying scientific materials, such as books, results, drawings, sketches -- copying and making data retrievable -- is what libraries ought to be well-equiped to do. However, as scientific publishing has become more digital, many of us are challenged with the question of where to host open data. We are hoping that open data publishers choose open, redundant software. We can get far by simply helping librarians achieve their goals. Academic journals, universities, and software foundations should start listening to trained digital librarians who have advocated for these open systems by supporting their creation and using them instead. Let's stop using proprietary services for data publishing, and instead advocate for open, redundant systems.

Karissa McKelvey<br>
Multiple hat wearer and Developer, Dat Project<br>
[http://karissa.github.io](http://karissa.github.io)<br>[@captainkmac on Twitter](http://twitter.com/captainkmac)

- [1] *If you want to read more about this topic, I highly recommend reading the work of the late Eleanor Ostrom and Charlotte Hess on commons management, [Understanding the Knowledge Commons](https://mitpress.mit.edu/books/understanding-knowledge-commons).*
